---
title: "Random Forests and Boosted Regression Trees - Homework"
author: "GROUP 4: Leonard Fidlin (h01352705), Daniel Jost (h01451889), Anne Valder (h11928415)"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: false
editor_options: 
  chunk_output_type: inline

---

```{r setup, include=FALSE}

knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = 'center')

```

<br>

**This assignment is due on December 7.**

**Please use the corresponding R Markdown file to save your results in an HTML file. Then upload the HTML file to learn.wu.ac.at.**

<br>


# Input data

Install and load the required packages.

```{r, echo=TRUE, results='hide', warning=FALSE, message=FALSE}

# Load packages
library(plyr)
library(tidyverse)
library(data.table)
library(fst)
library(ranger)
library(xgboost)
library(caret)
library(viridis)

```

Adjust the data path and load the input data.

```{r}

# Set wd before to use the relative path to the data
data_path = "."

# Read input data on municipal level for the whole country
col_input = read_fst(file.path(data_path, "data", "colombia_input.fst"))

# Read input data on grid level for the department Quindío
quindio_grid = read_fst(file.path(data_path, "data", "quindio_input_grid.fst"))

```

# Data Visualization and Exploratory Data Analysis

## Exercise 1

Use the input data for Colombia on the municipal level (colombia_input.fst) for this exercise.

### 1.1

Create a plot that shows the distribution of the population density (raster_pop_100). Briefly describe what you see.

```{r}

col_input %>% 
  ggplot(aes(x=raster_pop_100)) +
  geom_histogram(binwidth = 0.5)  +
  theme_classic()

col_input %>% 
  summarize(mean(raster_pop_100, na.rm = T), median(raster_pop_100, na.rm = T))

```
As can be seen in the plot, most grids have a rather low population density (below 10 inhabitants per 100m^2). In fact half of the grids have a population density below 0.44 inhabitants per 100m^2. To better visualize the distribution close to the origin we limit the scale of the x-axis to 10.

```{r}
col_input %>% 
  ggplot(aes(x=raster_pop_100)) +
  geom_histogram(binwidth = 0.05)  +
  scale_x_continuous(limit = c(0,10)) +
  theme_classic()

```

Here we can better see the distribution of the population density close to the origin (but do not see grids with a population density above 10 inhabitants per grid)

### 1.2

Create a plot that visualizes the relationship between the population density (raster_pop_100) and the light intensity recorded at night (night_lights_100). Combine two suitable geom functions in one plot. Briefly describe what you see.

```{r}

col_input %>% 
  ggplot(aes(x=raster_pop_100, y=night_lights_100, color = night_lights_100)) + 
  geom_point() + 
  geom_smooth() +
  theme_classic()

```
As expected, this plot suggests a positive and non-linear relationship of population density and night lights at the grid level. On average, a higher population density correlates with more light at night. The General Additive Model estimation also suggests that the relationship is marginally decreasing.

## Exercise 2

Use the grid-level input data for Quindío (quindio_input_grid.fst) for this exercise.

### 2.1

Create a plot that shows the distribution of the light intensity at night (night_lights_100). Include information on whether or not a grid lies inside a protected area (protected_areas_100). Briefly describe what you see.

Hint: Transform the variable protected_areas_100 to character inside the ggplot or the geom function.

```{r, warning=FALSE}


quindio_grid %>% 
  ggplot(aes(x=night_lights_100, fill = as.character(protected_areas_100))) +
  geom_histogram(binwidth = 0.3)  +
  theme_classic() + 
  scale_x_continuous(limits = c(0,20)) +
  scale_y_continuous(limits = c(0,10000)) +
  scale_fill_discrete(name = element_blank(), labels = c("Unprotected area", "Protected area"))

```
We can see that in protected areas there is also fewer nigh lights density. Furthermore, and unsurprisingly the distribution of night lights in protected areas looks similar to that of unprotected areas, being included within the other category.

### 2.2

Create a plot that visualizes the relationship between the light intensity at night (night_lights_100) and the slope (hydro_slo_100). What could be a problem with a standard scatterplot? Try to solve this issue and briefly describe what you see.

```{r}
plot(quindio_grid$night_lights_100, quindio_grid$hydro_slo_100)

quindio_grid %>% 
  ggplot(aes(x=night_lights_100,y = hydro_slo_100, color = night_lights_100)) + 
  geom_point() +
  theme_classic()
```
Both the base R and the ggplot standard scatterplot do not adequately visualize the distribution since n is very large. This is why we reduce the size of the data points and make them a bit transparent.

```{r}

quindio_grid %>% 
  ggplot(aes(x=night_lights_100,y = hydro_slo_100, color = night_lights_100)) + 
  geom_point(alpha = 1/10, size = 0.5) +
  theme_classic()

```
Now we can see that in high slope areas there are also mostly fewer night lights. We can see that night light dense areas tend to be flat. Reducing the size of the data points and their transparency indicates that there are obviously outliers, but they are much less prominent in the data.

# Random Forests and Boosted Regression Trees

## Exercise 3

### 3.1

Use the input data for Colombia on the municipal level (colombia_input.fst) to train a Random Forest. Use the log of the population density (raster_pop_100) as the dependent variable. Tune one of the parameters in the Random Forest. Briefly describe the parameter you are tuning and explain why you have decided for this parameter.


First,we want to use the the col_input data for all the estimations and tuning and then use quindio_grid for the predictions. Because once we find a good tuned random forest and boosted regression tree, we use the grid level data to estimate population density on the grid level. We begin by removing the first column in col_input containing the geocode. We can not use it for estimation, because it cannot help explain population density.

```{r}
col_input <- col_input[,2:76]
```
We are taking the log of the dependent variable because as was shown in class/publications this works better for estimations on population density

```{r}
col_input[,1] = log(col_input[,1])
```
# Since we want to tune the parameters we do not use the ranger function but a combination of the ranger and caret package. Alternatively we could also use the tune_ranger approach. The first step is to specify the type of cross validation we wan to do and the number of folds.  
```{r}
control = trainControl(method = "cv", number = 5) 
```
Next, we need a tuning grid, for which we specify the parameter we want to tune and the corresponding values which should be inspected. With caret and the ranger package we have 3 different parameters, which we can tune: Mtry, the splitrule and the min-nod-size. Following the lecture we decide that Mtry is most important and should therefore be tuned. Mtry is the number of variables used by rf to split the tree, and it cannot be less than one or more than the number of columns of predictors. As the criterion to be used to find the optimal split points we use variance. The minimal number of observations that are associated with each leave node has to be at least 1, since at least one observation has to be in each leaf node.

Considering the trade-off between how much parameters we want to try out and the time used to calculate the random forest, we decided to alter the maximum value for the sequence in mtry and the the number of min.node.size by 1, compared to the model in class, to obtain a tuning grid with 35 instead of 9 observations of 3 variables. This gives us a better chance of finding an even better final model. 
```{r}
tuning_grid = expand.grid(mtry = seq(20,50, by = 5), splitrule = "variance", min.node.size = seq(1, 5, by = 1))
head(tuning_grid)
```
We now use caret and ranger to optimize the model and tune the parameters. We also include the model's importance scores, which check how important each variable is in explaining our dependent variable.

```{r}
### HAT CA 15 Min zum rechnen bei mir gedauert!! aber dafür ists ein besseres model als in der VL! ### sonst b tuning grid wieder auf 30 by mytr und seq auf 2 setzen.

rf_caret = train(data = col_input, raster_pop_100 ~ ., method = "ranger", trControl = control, tuneGrid = tuning_grid, importance = "impurity")

rf_caret
rf_1 = rf_caret$finalModel


##### Alternative with the tuneRanger package #####
```
From the output of the caret training we chose the model with the smallest RMSE since this is the best one. In this case it is the model with (MTRY 30 / MIN.NOD.SIZE 30?) Then we save the final model from the caret package.  


### 3.2

Use the best Random Forest model from the tuning exercise to make a prediction of the grid-level population density in Quindío. Make sure that the sums of the grid-level predictions match the official numbers on the municipal level.

The Random Forest model can now be used to make a prediction. We will use grid-level data from Quindio, to estimate the city’s grid-level population density. First, we look at the variables in our data set. We use the columns with the explanatory variables in our Random Forest models to create predictions.

```{r}

# We only use the columns with the explanatory variables this is why we specify columns 5 to 78 

pred_rf_1 = predict(rf_1, data = quindio_grid [,5:78])

```
                    
To use the predictions for plots or analyses we first have to take the exponent, to reverse taking the log before the estimations. Moreover, we can combine them with information from the input data, like the coordinates of the corresponding grids, the geocodes, and the initial values of the average population density.

```{r}
pred_rf_1 = cbind(quindio_grid[,1:4], exp(pred_rf_1$predictions))

names(pred_rf_1) = c(names(quindio_grid[1:4]), "rf_1")

```

Finally, we want to make sure that in total our predictions match the official number on the municipal level. Hence, we rescale our numbers to the census data.

```{r}
pred_rf_1 = data.table(pred_rf_1)[, rf_1 := rf_1*sum(raster_pop_100)/sum(rf_1), by = raster_geocode_100]

```
## Exercise 4

### 4.1

Use the input data for Colombia on the municipal level (colombia_input.fst) to train a Boosted Regression Tree. Use the log of the population density (raster_pop_100) as the dependent variable. Tune one of the parameters in the Boosted Regression Tree. Briefly describe the parameter you are tuning and explain why you have decided for this parameter.

```{r}

# To optimize and tune  the model we combine the packages caret and xgboost.Again, we start this process by specifying which type of cross validation and number of folds we are interested in. The dependent variable is already in logs from the exercise before. 

control = trainControl(method = "cv", number = 5)

# Then we create a tuning grid, setting the parameters that we want to tune and the corresponding values that should be tried out. Here we can tune many different parameters. But again because of the time trade-off we only alter the nrounds (= maximum number of boosting iterations that should be done to create boosted regression tree), eta (= the learning rate between 0 and 1, which with lower values create a model that is more resistant to overfitting and therefore better) and gamma (= the minimum loss reduction that we then used to evaluate whether we want another split or not).

## parameter noch ändern!

tuning_grid = expand.grid(nrounds = seq(50, 150, by = 50), max_depth = 6, eta = seq(0.2, 0.4, by = 0.1), gamma = seq(0.01, 0.03, by = 0.01), colsample_bytree = 1, min_child_weight = 1, subsample = 1)

head(tuning_grid)
```

Then we combine the packages caret and xgboost to optimize the model doing extreme gradient boosting (using all variables as explanatory variables).

```{r, message=FALSE, warning=FALSE, results='hide'}

gb_caret = train(data = col_input, raster_pop_100 ~ ., method = "xgbTree", trControl = control, tuneGrid = tuning_grid)
```

```{r}

gb_caret

```

Again we choose the model with the smallest RMSE and extract and save it. The final values used for the model were nrounds = 150, max_depth = 6, eta = 0.2, gamma = 0.02, colsample_bytree = 1, min_child_weight = 1 and subsample = 1.

```{r}

gb_1 = gb_caret$finalModel

```
### 4.2

Use the best Boosted Regression Tree model from the tuning exercise to make a prediction of the grid-level population density in Quindío. Make sure that the sums of the grid-level predictions match the official numbers on the municipal level.

```{r}
# We can again use the columns with the explanatory variables [,5:78] in our data from Quinido grid level data set to create predictions of grid level population density in Quinido. 
 
pred_gb_1 = predict(gb_1, newdata = as.matrix(quindio_grid[,5:78]))

# We take the exponent again and combine the predictions with data on the coordinates of the grids, the geocodes, and the initial values of the average population density.

pred_gb_1 = cbind(quindio_grid[,1:4], exp(pred_gb_1))

# Adjusting the names
names(pred_gb_1) = c(names(quindio_grid[1:4]), "gb_1") 

# Making sure that in total our predictions match the official number on the municipal level.
pred_gb_1 = data.table(pred_gb_1)[, gb_1 := gb_1*sum(raster_pop_100)/sum(gb_1), by = raster_geocode_100]


#### Müssen wir und variable importance und RMSE im Vergleich anschauen ? 

```
# Visualization of Results

## Exercise 5

Create a plot showing the following three maps of the population density in Quindío side by side: one based on the input data, one based on your Random Forest prediction, and one based on your Boosted Regression Tree prediction. Try to make sure that the maps give a good picture of the differences in population density, both between the different data sets as well as between the different parts of the department.

```{r}
# First combine all predictions in one data frame to create one plot with all the predictions together to better compare the different predictions.

pred_all = join_all(list(pred_rf_1, pred_gb_1), by = c("x", "y", "raster_geocode_100", "raster_pop_100"))

# Transform the data set from wide to long format since than plotting in ggplot2 is easier.

pred_all = pivot_longer(pred_all, cols = c(raster_pop_100, rf_1, gb_1), names_to = "method", values_to = "prediction")

# In order to show the three maps of the population density in Quindío in the order we executed the estimation they have to be reordered.

pred_all$facet = factor(pred_all$method, levels = c("raster_pop_100", "rf_1", "gb_1"))

ggplot() +
    geom_raster(pred_all,mapping = aes(x = x ,y = y, fill = prediction)) +
    coord_fixed() +
    scale_fill_viridis(option ="B", trans = "log", breaks = c(3, 20, 150)) +
    scale_x_continuous(breaks = NULL) +
    scale_y_continuous(breaks = NULL) +
    facet_wrap(~ facet, nrow = 1) +
    labs(title = "Comparison of different maps of the popultation density")

```

We can see that mapping on the grid level with rf_1 and gb_1 gives us a better idea of the population dens ity on a more granular level. In the raster_pop_100 map we can only see the overall pop-density per municipality. For example in the southern part we see there is some higher population density we did not see before when just looking at population density on the municipal level. We can conclude that the population density is the highest in the upper center of Quindio.

