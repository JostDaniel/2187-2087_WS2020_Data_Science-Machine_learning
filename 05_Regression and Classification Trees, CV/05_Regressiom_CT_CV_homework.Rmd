---
title: "Regression and Classification Trees, Cross Validation - Homework"
author: "GROUP 4: Leonard Fidlin (h01352705), Daniel Jost (h01451889), Anne Valder (h11928415)"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: false
subtitle: Data Science and Machine Learning 2187 & 2087
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(fig.align = 'center')
```

# Classification Tree with the Voting Input data
The data contains prospective voting decisions in the 2017 parliamentary election in Austria as well as a set of predictors which correspond to different questions of the survey. 

First we install and load some required packages. 
```{r echo=T, message=FALSE, error=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot) 
library(precrec)
library(caret)       
library(party)
#library(MLmetrics)  
library(tidyverse)
library(vcd)
library(RColorBrewer)
#library(viridis)
library(knitr)
library(glmnet)     
```

Here we set the relative working directory and read in the voting data set. After inspecting the data we clean it by mutating and renaming some of the variables we want to use now & later on. On top, we change w1_q24 (prospective party choice) our dependent variable to a factor and rename it. 

```{r}
data_path = "."

voting <- read_delim(file.path(data_path, "voting.csv"), ";", 
                      escape_double = FALSE, col_types = cols(), 
                      locale = locale(decimal_mark = ",", grouping_mark = "."), trim_ws = TRUE)
summary(voting)

voting <- voting %>% mutate(gender = as.factor(na_if(gender, "Prefer not to say")), 
                        
                    male = recode(gender, "Male" = 1, "Female" = 0),
                    relig = na_if(w1_sd15r, "refuse"),
                    left_right = replace(w1_q12, w1_q12 > 10, NA), 
                    satis_gov = replace(w1_q7, w1_q7 > 5, NA), 
                    satis_democ = replace(w1_q6, w1_q6 > 5, NA), 
                    close_europe =  replace(w1_q8x4, w1_q8x4 > 5, NA), 
                    opinion_immigr = replace(w1_q10x1, w1_q10x1 > 5, NA), 
                    polit_trust = replace(w1_q19x3, w1_q19x3 > 5, NA), 
                    protect_environ = replace(w1_q17x4, w1_q17x4 > 5, NA), 
                    opinion_inequal = replace(w1_q5x1, w1_q5x1 > 5, NA), 
                    opinion_welf = replace(w1_q5x2, w1_q5x2 > 5, NA), 
                    opinion_unempl = replace(w1_q5x3, w1_q5x3 > 5, NA), 
                    opinion_pol = replace(w1_q5x4, w1_q5x4 > 5, NA), 
                    opinion_indiff = replace(w1_q5x5, w1_q5x5 > 5, NA), 
                    close_party_1 =replace(w1_q13, w1_q13 > 2, NA), 
                    financ = replace(w1_q4, w1_q4 > 5, NA), 
                    ft_voter = recode(w1_jw, "no" = 0, "yes" = 1),
                    vote = as.factor(w1_q24)) %>%
                    drop_na(male, relig) %>% 
                    rename("top_issue1" = w1_q2_level1, 
                          #"relig" = w1_sd15r,
                          "migration_pos" = w1_q10x2)
# evtl bei relig wieder ras und nur renmale!! csret model !

class(voting$vote)

# For the last task we here already specify the ÖVP dummy variable! notfalls altes model?

voting <- voting %>% mutate(vp = ifelse(vote == "ÖVP" , 1, 0), 
                    vp_vote = factor(vp, labels = c("No","Yes")))
```

# **Task 1** Classification tree using cross validation
**Estimate a classification tree using cross validation predicting the variable $w1_q24$ which is the prospective party choice.**

In order to estimate a classification tree we have different options. First, we will manually do cross validation. Later on, we will estimate a classification tree using the caret package and tune the parameters. The tree is a set of rules which will tell us voted for whom depending on the given criteria provided. 

We start by separating the voting data into train and test data. As usual the test data set is smaller because we do not want to lose a lot of observations when training the model, this also increases how our prediction works. 

```{r}
set.seed(123)

voting <- voting %>%
  mutate(train_index = sample(c("train", "test"), nrow(voting), replace=TRUE, prob=c(0.85, 0.15)))

train <- voting %>% filter(train_index=="train")
test <- voting %>% filter(train_index=="test")
```

Next, based on our expectations about what could most likely influence the prospective party choice we specify a formula and use the rpart package to create the tree from the test data. Thus, we partition the space spanned by input variables and fit a simple piecewise linear function. The main tuning parameter complexity is set arbitrarily to cp=.01. After estimating the tree we call the rpart-object to get the splits and terminal nodes of the tree. When specifying the formula (to estimate the response within a bin), we do not include all variables since this would likely lead to overfitting.

```{r}
formula <- vote ~ male + age + top_issue1 + financ + polit_trust + satis_gov + close_europe + left_right + migration_pos + relig
tree <- rpart(formula, data = train, cp=.01)
tree
```
From calling the rpart-object we see that we have a tree with 13 nodes, 7 terminal nodes and 6 splits. We start at the root node, which represents the entire population of the sample. From there we go down numerically to the next nodes. At the root node the tree splits using the criteria left_right>=4.5. This means if the left-right self-palcement is higher or equal to 5 the tree splits into the parent nodes SPÖ if "no" (=3)) or ÖVP if "yes" (=2)). From the ÖVP node the tree splits again with the close_europe criterion if lower than 2.5 we are in the terminal ÖVP node (=5)). Otherwise we are in the FPÖ node (=4)). Where we split into the terminal FPÖ node (=8)) and the ÖVP node (=9)) using again the satis_gov criterion being larger or equal to 3.5. In node 9) we have another split depending on the religion criterion. If relig=Christian-orthodox church,Islamic religious community,none,other we are in the terminal node FPÖ (=18)). If relig = relig=Protestant church,refused,Roman-Catholic church we end up with the terminal node ÖVP (=19)). Jumping back to the second parent node (SPÖ, 3)) the tree splits using the age citerion. If age is larger than 56.5 we are in the terminal node SPÖ (=7)). If age is smaller 56.5 we are in node SPÖ (=6)) where the tree splits again depending on migration_pos, if this is smaller 1.5 we are in terminal node Greens (12)) otherwise we are in SPÖ (13)).The types NEOS and other are unused in or tree.

## Extension with caret and CV

Now we turn to the estimation of the classification tree using the caret package. First, we use the trainControl function to specify how the samples should be split and which type of CV we want to apply. Here we use repeated cross validation, with 10 Kfolds repeating 10 times to reduce the variance of the performance measures we get. With summaryFunction we can specify some function that computes performance measures, which we can use for selecting the optimal parameters. Since we are having a multiclass problem here we use multiClassSummary.

```{r}
control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, savePredictions = T, classProbs = T, summaryFunction = multiClassSummary)
#control
#summary(control)
```

Next, we create a tuning grid that determines which values for the tuning parameters should be used. For every different combination of values train will perform cross validation. Here we only set the values for cp our cpmplexity parameter.

```{r}
# anpasssen nch ???
tuning_grid <- expand.grid(cp = seq(0, 0.03, by= 0.001))

tuning_grid
```

When we estimate the tree tuned with caret we get a similar tree to the one on the slides.

```{r, warning=FALSE}
#A:
tree_caret <- train(data = voting, formula , method = "rpart", trControl = control, tuneGrid = tuning_grid, metric = "ROC", na.action = na.pass)
tree_caret

#Alternative with  party package: 
#tuning_grid2 <- expand.grid(mincriterion = seq(0.02,0.20, by= 0.02))
#tree_caret2 <- train(data=voting, formula,method="ctree", trControl = control, tuneGrid = tuning_grid2, metric="ROC", na.action = na.pass)

```

When running this algorithm we get that the optimal cp = .009. This tells us that having arbitraily set the cp to 0.01 in our manual model was quite good already. 

# **Task 2** Tree graph
Plot a tree graph of the model with the best performance. Explain how you come to your prediction in one of the terminal nodes.

Vote overview
```{r}
table(voting$vote)
```

Colour palette for possible party outcomes
```{r}
party <- list("lightblue", "lightgreen", "turquoise", "pink")
```

To show the two different models we tried out so far we plot both graph. The the second tree, the caret tree, is of course performing better! 
```{r}
# Model based on "manual" tree
rpart.plot(tree, box.palette=party, nn=FALSE, type=2)
```

```{r}
# Model based on tree_caret
tree_caret_final <- tree_caret$finalModel
rpart.plot(tree_caret_final, box.palette=party, nn=FALSE, type=2)

```
Now look at the tree for using caret. As explained before when looking at the rpart-object we start in the root node (our positive class) and from there start going down to the next nodes concluding from the rules where the tree splits. As visulaized we end up with 10 terminal nodes and 9 splits. Here we also see the probability of the parties receiving a vote based on the splitting rules. For example take the terminal node to the right, SPÖ (11%), we get here by starting at the root node. The first split happens on whether the left_right self-placement is larger or equal to 5 if this is not the case (with a probability of 34% we are in the parent node SPÖ. From here if age is larger than 57 we end up in the terminal node SPÖ with a probability of 11%. 

**Task 3** Confusion matrix

In order to assess and compare the performance of the two models we make a confusion matrix. We use the test data since in the training data our model is always more accurate:

```{r}
# Model based on "manual" tree
test$prediction_tree <- predict(tree, newdata = test, type = c("class"))
confusion <- confusionMatrix(test$vote, test$prediction_tree,positive = "ÖVP", mode="sens_spec")
confusion
```

```{r}
# Model based on tree_caret

test$prediction_tree_caret <- predict.train(tree_caret, newdata = test, type = c("raw"), na.action = na.rpart)
confusion <- confusionMatrix(test$vote, test$prediction_tree_caret,positive = "ÖVP", mode="sens_spec")
confusion

```

Looking at the important measures of both models in the confusion matrix we see that in the model we estimated doing CV manually, both accuracy and kappa are as expected slightly smaller. This indicates that the model we estimated using the caret package performs better! Considering the sensitivity and specificity we also see that the tree_caret performs better in most of the classes than with the simple tree model. Furthermore, balanced accuracy improved for FPÖ. In general the accuracy and kappa for both models are not really high, with only about 0.5027 and 0.294 respectively. To get a better overview of the trade-off between the indicators of the confusion matrix we should look at ROC curves.

**Regarding which two categories do you find the highest and lowest sensitivity?**
We have the highest sensitivity or a vote outcome for FPÖ (model: tree_caret) with a probability of 63,64% and the lowest in SPÖ with a probability of 41,28%. 

**How well would you judge the predictive performance of the model?**
Given the accuracy rate and the kappa value the predictive performance of the model is not very good! (see above). Our tree just assigns a bit more than 50% of the votes to the true party. Also the no information rate is very high and clearly not significant from zero. Therefore, the tree does not estimate differently from a naive classifier.

**What would be the "naive prediction" in such a multiclass prediction problem?**
The naive prediction is that a voter votes for ÖVP (positive class).

# **Task 4** Large model
**Estimate the model across a wide range of parameters of the model and plot the accuracy in the training and in the test data against the parameters. What can you see? what problem does this relate to?**

In order to estimate the model across a wider range of parameters of the model we include a larger model and use differently tuned parameters. Since in our first model we already used a very small complexity parameter we show here that using a higher complexity parameter with caret leads to less accuracy and model performance. We then plot the training and test data of the first caret model with cp by 0.001. 

```{r}

control_new <- trainControl(method = "repeatedcv", number = 10, repeats = 10, savePredictions = T, classProbs = T, summaryFunction = multiClassSummary)
tuning_grid_new <- expand.grid(cp = seq(0, 0.01, by= 0.001))

tuning_grid_new
############

tree_caret_train <- train(data = train, formula , method = "rpart", trControl = control_new, tuneGrid = tuning_grid_new, metric = "ROC", na.action = na.pass)
tree_caret_train

tree_caret_test <- train(data = test, formula , method = "rpart", trControl = control_new, tuneGrid = tuning_grid_new, metric = "ROC", na.action = na.pass)
tree_caret_train

plot(tree_caret_train)
plot(tree_caret_test)
##############


#train data

train$prediction_tree_large <- predict(tree_large, newdata = train, type = c("class"
))  
 
train <- train %>%  
  drop_na(prediction_tree_large) 
 
confusion <- confusionMatrix(train$vote,train$prediction_tree_large) 
confusion

# test data

test$prediction_tree_large <- predict(tree_large, newdata = test, type = c("class"))  
 
test <- test %>%  
  drop_na(prediction_tree_large) 
 
confusion <- confusionMatrix(test$vote,test$prediction_tree_large) 
confusion

########################

#PLOT????

```
We can see that when we estimate the model across a wide range of parameters and then plot the accuracy in the training and in the test data against the parameters that we have a much better accuracy for the new larger model in the training data. Using the test data however shows us that we have a lower accuracy than in the first models. This comes from overfitting the model and thus leads to less predicitive power.

# **Task 5** Simplified model
**Simplify the model by only trying to predict if somebody voted for the ÖVP or not, re-estimate the model (again by using CV) as well as estimating a logit model in the training data utilizing whatever variables you find appropriate**

The new dummy variable vp which takes the value 1 for ÖVP and 0 otherwise we use here was already created in the beginning. We have a binary classification instead of the multi-class version here. First we specify a new formula and then use rpart to estimate the tree, then we compute the confusion model. Second, we esimate a logit model and also compute the confusion matrix to compare the two models and their performance.

```{r}


baum plotten noch!! pred tree 
  
 Andere formel?
  
formula_vp <-  vp_vote ~ age + male + satis_democ + left_right
tree_vp <- rpart(formula_vp, data = train, cp = 0.005)


test$prediction_tree_vp <- predict(tree_vp, newdata = test, type = c("class"))  
 
test <- test %>%  
  drop_na(prediction_tree_vp) 
 
confusion <- confusionMatrix(test$vp_vote,test$prediction_tree_vp) 
confusion
```

# Logit model

```{r}
logit_vp <- glm(formula_vp, 
             data = train, family = "binomial")

test$prediction_logit <- predict(logit_vp, newdata = test, type = "response")

test <- test %>%  
  mutate(prediction_logit=as.factor(ifelse(prediction_logit>0.5,"Yes","No"))) %>%  
  drop_na(prediction_logit) 

confusion <- confusionMatrix(test$vp_vote, test$prediction_logit)
confusion



text noch logit sollte logit schkelchter sein .... variablen am anfang... 
```

# **Task 6** Confusion matrix and ROC-curves
**Compute the confusion matrix and ROC-curves for both models. Which one performs better and why? Where would you set the cut-off-point?** 
Confusion Matrix see before.

ROC
```{r}

test$prediction_tree_scores <- predict(tree_vp, test ,type = c("prob"))[,2] 
test$prediction_logit_scores <- predict(logit_vp, test ,type = c("response")) 
 
precrec_obj <- evalmod(scores = cbind(test$prediction_tree_scores, test$prediction_logit_scores), labels = cbind(test$vp_vote,test$vp_vote),modnames = c("classification tree","logit"), raw_curves = FALSE, ties_method="first")

autoplot(precrec_obj)

```

####Interpretation fehlt noch bei 6 & 5 teils !!!!!#####

cutoff point - weiteste entfernung! 

