---
title: "Regression and Classification Trees, Cross Validation - Homework"
author: "GROUP 4: Leonard Fidlin (h01352705), Daniel Jost (h01451889), Anne Valder (h11928415)"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: false
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}

knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(fig.align = 'center')

```

# Classification Tree with the Voting

# Input data

Install and load the required packages.
```{r echo=T, message=FALSE, error=FALSE, warning=FALSE}

# Später schauen ob wir alle aus der VL brauchen ! 

library(rpart)
library(rpart.plot) 
library(precrec)
library(caret)## funktioniert beir mir nicht
library(party)
library(MLmetrics) #brauchen wir für Caret
library(libcoin) # noch dazu ?

library(tidyverse)
library(vcd)
library(RColorBrewer)
library(viridis)
library(knitr)
library(tinytex)
library(glmnet) ## funktioniert beir mir nicht

# Set wd to Set the path to the data
data_path = "."

voting <- read_delim(file.path(data_path, "voting.csv"), ";", 
                      escape_double = FALSE, col_types = cols(age = col_number()), 
                      locale = locale(decimal_mark = ",", grouping_mark = "."), trim_ws = TRUE)

# Change Vote to a factor
voting <- voting %>% mutate(vote_outcome = as.factor(w1_q24))

# Changing the Gender Variable and removing "wrong" survey entries in the used Variables

# Diser Schritt ist von Julian und macht auch eigentlich Sinn, da seltsam ausgefüllte Antworten mit 99 oder 88 gekennzeichnet wurden und damit diese nicht ein so hohe Gewichtung bekommen entferne ich sie hier, aber das Ergebnis sieht dann leider ganz anders aus als aus der Vorlesung :/

# voting <- voting %>% mutate(gen = as.factor(gender),
#                            female = recode(gender, "Male" = 0, "Female" = 1),
#                            left_or_right = replace(w1_q12, w1_q12 > 10, NA),
#                            gov_sat = replace(w1_q7, w1_q7 > 5, NA),
#                            financial_sit = replace(w1_q4, w1_q4 > 5, NA),
#                            close_to_eu = replace(w1_q8x4, w1_q8x4 > 5, NA),
#                            immigration_pos = replace(w1_q10x2, w1_q10x2 > 5, NA),
#                            polit_trust = replace(w1_q19x3, w1_q19x3 > 5, NA),
#                            immigration_neg = replace(w1_q17x3, w1_q17x3 > 5, NA))

# Changing the names
voting <- voting %>%
  rename("top_issue1" = w1_q2_level1,
         "religious_aff" = w1_sd15r,
         "educ" = w1_sd6,
         "income" = w1_sd7,
         "left_or_right" = w1_q12,
         "gov_sat" = w1_q7,
         "financial_sit" = w1_q4,
         "close_to_eu" = w1_q8x4,
         "immigration_pos" = w1_q10x2,
         "polit_trust" = w1_q19x3,
         "immigration_neg" = w1_q17x3)
```

# Task 1) Estimate a classification tree using cross validation predicting the variable w1_q24

```{r }
# manuell und wenn ja welche model specification oder mit caret??? - So wie es ist, ist es ohne CV???
set.seed(123)

# Seperate the voting data set into test and training data
voting <- 
  voting %>%
  mutate(train_index = sample(c("train", "test"), nrow(voting), replace=TRUE, prob=c(0.75, 0.25)))

voting_train <- voting %>% filter(train_index=="train")
voting_test <- voting %>% filter(train_index=="test")


# The question here should be whether we are pre-pruning or post-pruning 'formula' is a rather simple model while 'formula1' is a bit more complex but far away from representing all the 63 possible variables. My computer is not capable of using all the variables, the ones included in 'formula1'  are already close to the maximum my PC can handle. I have removed some, because they did not change anything in the outcome. The resulting tree in 'tree_caret' looks very similar to the one presented in the Lecture!

#For simplicity use formula for estimation
formula <- vote_outcome ~  gender + age + close_to_eu + left_or_right + immigration_neg

# The inclusion of the Educ Variable is quite questionable at this point ^^
formula1 <- vote_outcome ~  gender + age + top_issue1 + financial_sit + polit_trust + gov_sat + close_to_eu + income + left_or_right + immigration_neg + immigration_pos + religious_aff

tree <- rpart(formula, data = voting_train, cp=.01)
tree
summary(tree)


tree1 <- rpart(formula1, data = voting_train, cp=.01)
tree1
```

```{r}
###### Erweiterung mit caret und CV #####

# When running this algorithm we get that the optimal cp = .007 / .006. However, after running the control estimate we can no longer also generate the Confusion Matrix due to some Variable missing. Not so sure as of yet how to solve it. 


control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, savePredictions = T, classProbs = T, summaryFunction = multiClassSummary)
control
summary(control)

# Tuning Grid
tuning_grid <- expand.grid(cp = seq(0.001, 0.01, by= 0.001))
tuning_grid


# Tree tuned with Caret, we get a similar tree to the one in the lecture
tree_caret1 <- train(data = voting, formula1, method = "rpart", trControl = control, tuneGrid = tuning_grid, metric = "ROC", na.action = na.pass)
tree_caret1

tree_caret1 <- tree_caret1$finalModel

# The caret method tells us that we should use cp = 0.006 for estimation.
```
```{r}
# I expected it too look like the one we got from Caret but it does not
tree2 <- rpart(formula1, data = voting_train, cp=.006)

tree2

```

```{r}
# Vote overview
table(voting$vote_outcome)

# Colour palette for possible party outcomes?
# cols <- c("blue", "green", "pink", "gray", "black", "red")
```

#Task 2) Plot a tree graph of the model with the best performance. Explain how you come to your prediction in one of the terminal nodes

```{r}
# Tree plot Formula
rpart.plot(tree, box.palette="RdYlGn", nn=FALSE, type=2)

# Tree plot more variables Formula1
rpart.plot(tree1, box.palette="RdYlGn", nn=FALSE, type=2)

# Tree plot, CV Caret Tree, many variables Formula1
rpart.plot(tree_caret1, box.palette="RdBu", nn=FALSE, type=2)

# Tree plot, Rpart with the new CP
rpart.plot(tree2, box.palette="RdYlGn", nn=FALSE, type=2)

```

#Task 3) Make a confusion matrix of the model and interpret the results. 
# 3.1) Regarding which two categories do you find the highest and lowest sensitivity?
# 3.2) How well would you judge the predictive performance of the model?
# 3.3) What would be the "naive prediction" in such a multiclass prediction problem?

```{r}
# Earlier it worked but now an Error message pops up, Im not sure why tho? 
# We can use this for the normal Tree1, but not for the tree_caret1 tuned version, not clear why

voting_train$prediction_tree <- predict(tree1, newdata = voting_train, type = "class") 

# We have to change the level of the vote_outcome variable to be the same as the outcome of the prediction
# voting_train$vote_outcome <- voting_train$vote_outcome %>% as.factor()

# 1st Confusion matrix
confusion1 <- confusionMatrix(voting_train$vote_outcome , voting_train$prediction_tree)
confusion1

# 2nd Confusion matrix
voting_test$prediction_tree <- predict(tree1, newdata = voting_test, type = "class")

confusion <- confusionMatrix(voting_test$vote_outcome, voting_test$prediction_tree)
confusion



#Confusion Matrix for Tree2
voting_train$prediction_tree2 <- predict(tree2, newdata = voting_train, type = "class")
confusion2 <- confusionMatrix(voting_train$vote_outcome , voting_train$prediction_tree2)
confusion2

voting_test$prediction_tree2 <- predict(tree2, newdata = voting_test, type = "class")
confusion2_1 <- confusionMatrix(voting_test$vote_outcome , voting_test$prediction_tree2)
confusion2_1

# This prediction has very low accuracy, we have the highest sensitivity for a vote outcome for FPÖ and the highest specicivity for "Ohter".

# The predictove performance of the model is not very good!
# The naive prediction is that ÖVP would win

# 3rd Confusion Matrix - does not work again because of the Tree Caret issue
voting_train$prediction_tree_caret1 <- predict(tree_caret1, newdata = voting_train, type = "class")

voting_test$vote_outcome <- voting_test$vote_outcome%>%
  as.factor()

confusion2 <- confusionMatrix(voting_test$vote_outcome, voting_test$prediction_tree)
confusion2
```

#Task 4) Estimate the model across a wide range of parameters of the model and plot the accuracy in the training and in the test data against the parameters. What can you see? what problem does this relate to?

```{r}

# Both have same Level but can not run the confusion matrix, weird
logit <- glm(formula1, 
             data = voting_train, family = "binomial")

voting_test$prediction_logit <- predict(logit, newdata = voting_test, type = "response")

confusion <- confusionMatrix(voting_test$vote_outcome, voting_test$prediction_logit)
confusion

levels(voting_test$prediction_logit)
```

#Task 5) Simplify the model by only trying to predict if somebody voted for the ÖVP or not,
re-estimate the model (again by using CV) as well as estimating a logit model in the training
data utilizing whatever variables you find appropriate

```{r}

```

#Task 6) Compute the confusion matrix and ROC-curves for both models. Which one performs better and why? Where would you set the cut-off-point? 

```{r}

```




