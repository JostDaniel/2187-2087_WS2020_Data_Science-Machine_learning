---
title: "Regression and Classification Trees, Cross Validation - Homework"
author: "GROUP 4: Leonard Fidlin (h01352705), Daniel Jost (h01451889), Anne Valder (h11928415)"
output: 
  html_document:
    toc: true
    toc_depth: 2
    number_sections: false
subtitle: Data Science and Machine Learning 2187 & 2087
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(include = TRUE)
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
knitr::opts_chunk$set(fig.align = 'center')
```

# Classification Tree with the Voting Input data
Data contains prospective voting decisions in the 2017 parliamentary election in Austria as well as a set of predictors which correspond to other questions of the survey. 

First we install and load the required packages. 
```{r echo=T, message=FALSE, error=FALSE, warning=FALSE}
library(rpart)
library(rpart.plot) 
library(precrec)
library(caret)       
library(party)
library(MLmetrics)  
#library(libcoin)    
library(tidyverse)
library(vcd)
library(RColorBrewer)
library(viridis)
library(knitr)
#library(tinytex)
library(glmnet)     
```

Here we set the relative working directory and read in the voting data-set. 
Next, after inspecting the data we mutate and rename some of the variables we want to use now & later on. On top, we change w1_q24 (prospective party choice) our dependent variable to a factor and rename it. 

```{r}
data_path = "."

voting <- read_delim(file.path(data_path, "voting.csv"), ";", 
                      escape_double = FALSE, col_types = cols(), 
                      locale = locale(decimal_mark = ",", grouping_mark = "."), trim_ws = TRUE)


summary(voting)

voting <- voting %>% mutate(gender = as.factor(na_if(gender, "Prefer not to say")), 
                    male = recode(gender, "Male" = 1, "Female" = 0), 
                    left_right = replace(w1_q12, w1_q12 > 10, NA), 
                    satis_gov = replace(w1_q7, w1_q7 > 5, NA), 
                    satis_democ = replace(w1_q6, w1_q6 > 5, NA), 
                    close_europe =  replace(w1_q8x4, w1_q8x4 > 5, NA), 
                    opinion_immigr = replace(w1_q10x1, w1_q10x1 > 5, NA), 
                    polit_trust = replace(w1_q19x3, w1_q19x3 > 5, NA), 
                    protect_environ = replace(w1_q17x4, w1_q17x4 > 5, NA), 
                    opinion_inequal = replace(w1_q5x1, w1_q5x1 > 5, NA), 
                    opinion_welf = replace(w1_q5x2, w1_q5x2 > 5, NA), 
                    opinion_unempl = replace(w1_q5x3, w1_q5x3 > 5, NA), 
                    opinion_pol = replace(w1_q5x4, w1_q5x4 > 5, NA), 
                    opinion_indiff = replace(w1_q5x5, w1_q5x5 > 5, NA), 
                    close_party_1 =replace(w1_q13, w1_q13 > 2, NA), 
                    close_party_2 =replace(w1_q14, w1_q14 > 2, NA), 
                    ft_voter = recode(w1_jw, "no" = 0, "yes" = 1),
                    vote = as.factor(w1_q24)) %>%
                    drop_na(male) %>% 
                    rename("top_issue1" = w1_q2_level1, 
                          "relig" = w1_sd15r,
                          "mig_pos" = w1_q10x2)

class(voting$vote)

# For the last task we here already specify the ÖVP dummy variable!


voting <- voting %>% mutate(vp = ifelse(vote == "ÖVP" , 1, 0), 
                    vp_vote = factor(vp, labels = c("No","Yes")))

#FORMULA <- vote ~ gender + age + top_issue1 + financ + polit_trust + gov_sat + close_eu + income + left_right + migration_neg + migration_pos + relig




#voting <- voting %>% mutate(vote = as.factor(w1_q24))


# Changing the names
#voting <- voting %>%
#  rename("top_issue1" = w1_q2_level1,
 #        "relig" = w1_sd15r,
  #       "educ" = w1_sd6,
   #      "income" = w1_sd7,
    #     "left_right" = w1_q12,
     #    "gov_sat" = w1_q7,
      #   "financ" = w1_q4,
       #  "satisfaction_democ" = w1_q6,
        # "close_eu" = w1_q8x4,
         #"migration_pos" = w1_q10x2,
         #"polit_trust" = w1_q19x3,
         #"migration_neg" = w1_q17x3)
```

# **Task 1** Classification tree using cross validation
**Estimate a classification tree using cross validation predicting the variable $w1_q24$ which is the prospective party choice.**

In order to estimate a classification tree we have different options. First, we will manually do cross validation. Later on, we will estimate a classification tree using the caret package. The tree is a set of rules which will tell us voted for whom depending on the given criteria provided. 

We start by separating the voting data into train and test data. As usual the test data set is smaller because we do not want to lose a lot of observations when training the model, this also increases how our prediction works. After splitting the data set

```{r}
set.seed(123)

voting <- voting %>%
  mutate(train_index = sample(c("train", "test"), nrow(voting), replace=TRUE, prob=c(0.85, 0.15)))

train <- voting %>% filter(train_index=="train")
test <- voting %>% filter(train_index=="test")
```

Next, we specify a formula and use the rpart package to create the tree from the test data. Thus, we partition the space spanned by input variables and fit a very simple piecewise linear function. The main tuning parameter complexity is set arbitrarily to cp=.005. After estimating the tree we call the rpart-object to get the splits and terminal nodes of the tree. When specifying the formula (to estimate the response within a bin), we do not include all variables since this would likely lead to overfitting.

```{r}

formula <- vote ~ age + male + satis_democ + left_right
tree <- rpart(formula, data = train, cp=.005)
tree

```
From calling the rpart-object we see that we have a tree with 17 nodes, 9 terminal nodes and 8 splits. We start at the root node, which represents the entire population of the sample. From there we go down numerically to the next nodes. At the root node the tree splits using the criteria left_right>=4.5. This means if the left-right self-palcement is higher or equal to 5 (indicating...) the tree splits into the parent nodes SPÖ if "no" (=3)) or ÖVP if "yes" (=2)). From the ÖVP node the tree splits again with the close_europe criterion if lower than 2.5 we are in the terminal ÖVP node (=5)). Otherwise we are in the FPÖ node (=4)). Where we split into the terminal FPÖ node (=8)) and the ÖVP node (=9)) using again the satis_gov criterion being larger or equal to 3.5. In node 9) we have another split depending on the left_right selfplacement criterion. If it is smaller 5.5 we end in the terminal node ÖVP (=19)) otherwise if  greater or equal to 5.5 we end in the FPÖ node (=18)), splitting again into FPÖ (36)) and ÖVP (37)) depending on age being smaller 53.5. Jumping back to the second parent node (SPÖ, 3)) the tree splits using the age citerion. If age is larger than 56.5 we are in the terminal node SPÖ (=7)). If age is smaller 56.5 we are in node 6) where the tree splits again depending on opinion_immgr, if this is smaller 1.5 we are in terminal node ÖVP (13)) otherwise we are in Greens (12)) where one last split occurs given the satis_gov criterion. If satis_gov is smaller 2.5 we end in the terminal node SPÖ (=25)), otherwise in the terminal node Greens (=24)).The types NEOS and other are unused in or tree. 

## Extension with caret and CV

Now we turn to the estimation of the classification tree using the caret package. First, we use the trainControl function to specify how the samples should be split and which type of CV we want to apply. Here we use repeated cross validation, with 10 Kfolds repeating 10 times to reduce the variance of the performance measures we get. With summaryFunction we can specify some function that computes performance measures, which we can use for selecting the optimal parameters. Since we are having a multiclass problem here we use multiClassSummary.

```{r}
control <- trainControl(method = "repeatedcv", number = 10, repeats = 10, savePredictions = T, classProbs = T, summaryFunction = multiClassSummary)
#control
#summary(control)
```

Next, we create a tuning grid that determines which values for the tuning parameters should be used. For every different combination of values train will perform cross validation. Here we only set the values for cp.

```{r}
tuning_grid <- expand.grid(cp = seq(0, 0.04, by= 0.005))
tuning_grid
```

When we estimate the tree tuned with caret we get a similar tree to the one on the slides.

```{r, warning = false}
#A:
tree_caret <- train(data = voting, formula , method = "rpart", trControl = control, tuneGrid = tuning_grid, metric = "ROC", na.action = na.pass)
tree_caret

#Alternative with  party package: 
#tuning_grid2 <- expand.grid(mincriterion = seq(0.02,0.20, by= 0.02))
#tree_caret2 <- train(data=voting, formula,method="ctree", trControl = control, tuneGrid = tuning_grid2, metric="ROC", na.action = na.pass)

```

When running this algorithm we get that the optimal cp = .005. 

# **Task 2** Tree graph
Plot a tree graph of the model with the best performance. Explain how you come to your prediction in one of the terminal nodes.

Vote overview
```{r}
table(voting$vote)
```

Colour palette for possible party outcomes
```{r}
party <- list("lightblue", "lightgreen", "turquoise", "pink")
```

```{r}
# Model based on "manual" tree
rpart.plot(tree, box.palette=party, nn=FALSE, type=2)
```

```{r}
# Model based on tree_caret
tree_caret_final <- tree_caret$finalModel
rpart.plot(tree_caret_final, box.palette=party, nn=FALSE, type=2)

```
Now look at the tree for using caret. As explained before when looking at the rpart-object we start in the root node (our positive class) and from there start going down to the next nodes concluding from the rules where the tree splits. As visulaized we end up with 10 terminal nodes and 9 splits. Here we also see the probability of the parties receiving a vote based on the splitting rules. For example take the terminal node SPÖ (11%) we get here by starting at the root node. The first split happens on whether the left_right self-placement is larger or equal to 5 if this is not the case (with a probability of 34% we are in the parent node SPÖ. From here if age is smaller 57 we end up in the terminal node SPÖ with a probability of 11%. 


# **Task 3** Confusion matrix

In order to assess and compare the performance of the two models we make a confusion matrix. We use the test data since in the training  data our model is always more accurate:

```{r}
# Model based on "manual" tree

test$prediction_tree <- predict(tree, newdata = test, type = c("class"))
confusion <- confusionMatrix(test$vote, test$prediction_tree,positive = "ÖVP", mode="sens_spec")
confusion
```

```{r}
# Model based on tree_caret

test$prediction_tree_caret <- predict.train(tree_caret, newdata = test, type = c("raw"), na.action = na.rpart)
confusion <- confusionMatrix(test$vote, test$prediction_tree_caret,positive = "ÖVP", mode="sens_spec")
confusion


# Alternative mit party package 
#test$prediction_tree_caret2 <- predict.train(tree_caret2, newdata = test, type = c("raw"), na.action = na.rpart)
#confusion <- confusionMatrix(test$vote, test$prediction_tree_caret2,positive = "ÖVP", mode="sens_spec")
#confusion

```

Looking at the important measures of both models in the confusion matrix we see that in the model we estimated doing CV manually accuracy, kappa are as expected both slightly smaller. This indicates that the model we estimated using the caret package performs better! Considering the sensitivity and specificity we also see that the tree_caret performs better in the classes Greens, ÖVP and SPÖ than with the simple tree model. Also balanced accurcy improved in all classes.But in general in both models accuracy and kappa are not really high with only about 0.5027 and 0.3008. To get a better overview of the trade-off between the indicators of the confusion matrix we should look at ROC curves.

**Regarding which two categories do you find the highest and lowest sensitivity?**
We have the highest sensitivity or a vote outcome for FPÖ (model: tree_caret) with a probability of 57,89% and the lowest in Greens with a probability of 36,67%. 

**How well would you judge the predictive performance of the model?**
Given tha accuracy rate andf the kappa value the predictive performance of the model is not very good! (see above). Our tree just assigns a bit more than 50% of the votes to the true party. Also the no information rate is very high and clearly not significant to zero, therefore the tree does not estimate differently from a naive classifier.

**What would be the "naive prediction" in such a multiclass prediction problem?**
The naive prediction is that a voter votes for ÖVP (positive class).

# **Task 4** Large model
**Estimate the model across a wide range of parameters of the model and plot the accuracy in the training and in the test data against the parameters. What can you see? what problem does this relate to?**

In order to estimate the model across a wider range of parameters of the model we include a larger model. Therefore however we use a weaker tunning parameter, as a penality for including more parameters.

```{r}
formula_large <- vote ~ male + age + left_right + satis_democ + satis_gov + protect_environ  + close_europe + opinion_immigr + polit_trust + opinion_inequal + opinion_welf + opinion_unempl + opinion_pol + opinion_indiff + close_party_1 +close_party_2 + ft_voter

tree_large <- rpart(formula_large , data = train, cp=.001)
tree_large
rpart.plot(tree_large, box.palette=party, nn=FALSE, type=2)

#train data

train$prediction_tree_large <- predict(tree_large, newdata = train, type = c("class"
))  
 
train <- train %>%  
  drop_na(prediction_tree_large) 
 
confusion <- confusionMatrix(train$vote,train$prediction_tree_large) 
confusion

# test data

test$prediction_tree_large <- predict(tree_large, newdata = test, type = c("class"))  
 
test <- test %>%  
  drop_na(prediction_tree_large) 
 
confusion <- confusionMatrix(test$vote,test$prediction_tree_large) 
confusion

########################

#PLOT????

```
We can see that when we estimate the model across a wide range of parameters and then plot the accuracy in the training and in the test data against the parameters that we have a much better accuracy for the new larger model in the training data. Using the test data however shows us that we have a lower accuracy than in the first models. This comes from overfitting the model and thus leads to less predicitive power.

# **Task 5** Simplified model
**Simplify the model by only trying to predict if somebody voted for the ÖVP or not, re-estimate the model (again by using CV) as well as estimating a logit model in the training data utilizing whatever variables you find appropriate**

The new dummy variable vp which takes the value 1 for ÖVP and 0 otherwise we use here was already created in the beginning. We have a binary classification instead of the multi-class version here. First we specify a new formula and then use rpart to estimate the tree, then we compute the confusion model. Second, we esimate a logit model and also compute the confusion matrix to compare the two models and their performance.

```{r}

formula_vp <-  vp_vote ~ age + male + satis_democ + left_right
tree_vp <- rpart(formula_vp, data = train, cp = 0.01)


test$prediction_tree_vp <- predict(tree_vp, newdata = test, type = c("class"))  
 
test <- test %>%  
  drop_na(prediction_tree_vp) 
 
confusion <- confusionMatrix(test$vp_vote,test$prediction_tree_vp) 
confusion
```

# Logit model

```{r}
logit_vp <- glm(formula_vp, 
             data = train, family = "binomial")

test$prediction_logit <- predict(logit_vp, newdata = test, type = "response")

test <- test %>%  
  mutate(prediction_logit=as.factor(ifelse(prediction_logit>0.5,"Yes","No"))) %>%  
  drop_na(prediction_logit) 

confusion <- confusionMatrix(test$vp_vote, test$prediction_logit)
confusion

```

# **Task 6** Confusion matrix and ROC-curves
**Compute the confusion matrix and ROC-curves for both models. Which one performs better and why? Where would you set the cut-off-point?** 
Confusion Matrix see before.

ROC
```{r}

test$prediction_tree_scores <- predict(tree_vp, test ,type = c("prob"))[,2] 
test$prediction_logit_scores <- predict(logit_vp, test ,type = c("response")) 
 
precrec_obj <- evalmod(scores = cbind(test$prediction_tree_scores, test$prediction_logit_scores), labels = cbind(test$vp_vote,test$vp_vote),modnames = c("classification tree","logit"), raw_curves = FALSE, ties_method="first")

autoplot(precrec_obj)

```

####Interpretation fehlt noch bei 6 & 5 teils !!!!!#####



